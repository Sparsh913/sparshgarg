<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-60320583-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-60320583-2');
</script>

<title>Sparsh Garg - Home</title>
<link rel="stylesheet" type="text/css" href="style.css">

<script type="text/javascript" src="js/hidebib.js">
  function toggleProjects() {
    var projectList = document.getElementById("projects-list");
    if (projectList.style.display === "block") {
      projectList.style.display = "none";
    } else {
      projectList.style.display = "block";
    }
  }
</script>

</head>
<body>

<div class="section">
  <h1>Sparsh Garg</h1>
</div>
<hr>

<div class="section">
  <table>
    <tr valign="top"> 
      <td style="width: 620px; vertical-align: top;">
        I am currently pursuing a Masterâ€™s degree at Carnegie Mellon University, specializing in computer vision and robotics. Under the mentorship of Dr. George A. Kantor at the Robotics Institute's Kantor Lab, I am engaged in advancing automation technologies for agriculture. My work focuses on developing advanced perception and manipulation pipelines for various downstream appliactions including agriculture.
        <p><br></p>
        I have also been working as a Research Intern at Bosch Research (<a href="https://www.bosch-ai.com/">BCAI</a>), Sunnyvale. At Bosch, I have been involved in developing state-of-the-art model for monocular metric depth estimation from any camera.
        <p><br></p>
        Prior to my current pursuit, I earned my undergraduate degree from Punjab Engineering College, India. Subsequently, I gained valuable professional experience working at <a href="https://corporate.exxonmobil.com/">ExxonMobil</a> in Bangalore. Following this, I served as a Design & Development Engineer in the RnD team of Cybernetics Laboratory (<a href="https://www.cynlr.com/">CynLr</a>), Bangalore. At CynLr, I played a key role in developing a stereo vision system from the ground up, contributing to both the hardware and software layers of the first version of the product.
        <p><br></p>
        My overarching goal is to contribute to the realization of truly autonomous systems. Recognizing the pivotal role of vision in achieving this objective, I have dedicated my master's program to in-depth study and research in the field of computer vision. My research interests span computer vision, deep learning, and robotics.
        <p><br></p>
        I am enthusiastic about leveraging my skills and knowledge to advance the frontier of autonomous technologies and make a meaningful impact on the field.
        <p><br></p>
        I am actively looking for full-time opportunities in the field of robotics and computer vision. If you think I can be a good fit for your team, please feel free to reach out to me.
        <!-- <p><br></p> --> </td>
        <!-- <p> -->
          <td width="400" style="vertical-align: top;">
            <div style="text-align: right;">
              <img src="img.jpg" alt="My picture" height="250" style="border-radius: 15px;" />
              <br><br>
              <ul style="list-style-type: none; padding: 0; margin: 0; text-align: right;">
                <li><a href="javascript:toggleblock('contact')">Contact</a></li>
                <li><a href="https://www.linkedin.com/in/sparsh-g/" target="_blank">LinkedIn</a></li>
                <li><a href="https://scholar.google.com/citations?user=uP6sOhoAAAAJ&hl=en" target="_blank">Google Scholar</a></li>
                <li><a href="https://github.com/Sparsh913" target="_blank">GitHub</a></li>
                <li><a href="https://x.com/_sparshgarg_" target="_blank">X</a></li>
                <li><a href="https://drive.google.com/file/d/1HkE89Ar5pzVF-pmAOr3YzrINoPfqIdr9/view?usp=sharing" target="_blank">CV</a></li>
                <li><a href="javascript:toggleProjects()">Projects</a></li>
              </ul>
              <!-- <li><a href="javascript:void(0)" onclick="toggleProjects()">Projects</a></li> -->
              <div id="projects-list" class="dropdown hidden">
                <a href="dac.html">Depth Any Camera</a>
                <a href="splatsim.html">SplatSim</a>
                <a href="vine_stereo.html">Vine Stereo</a>
                <a href="fluid.html">Liquid Segmentation and Viscosity Estimation</a>
                <a href="prosthetic_arm.html">EMG Prosthetic Arm</a>
              </div>
              <pre xml:space="preserve" id="contact" style="font-size: 15px; line-height: 1.8;">
    email: sparshg@andrew.cmu.edu
    Office: CIC Robotics Institute (near Lab 2)
              </pre>
              <script xml:space="preserve" language="JavaScript">
                hideblock('contact');
              </script>
            </div>
          </td>
        </tr>
      </table>
    </div>

<hr>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    const projectToggle = document.querySelector('a[href="javascript:toggleProjects()"]');
    const projectList = document.getElementById("projects-list");
  
    // Ensure dropdown is hidden when page loads
    projectList.classList.add("hidden");
  
    // Toggle projects dropdown
    function toggleProjects() {
      projectList.classList.toggle("hidden");
    }
  
    // Add click event to the toggle link
    projectToggle.addEventListener('click', function(event) {
      event.stopPropagation(); // Prevent this click from immediately closing the dropdown
      toggleProjects();
    });
  
    // Close dropdown when clicking outside
    document.addEventListener('click', function(event) {
      // Check if the click is outside the dropdown and the toggle link
      if (!projectList.contains(event.target) && 
          !projectToggle.contains(event.target) && 
          !projectList.classList.contains("hidden")) {
        projectList.classList.add("hidden");
      }
    });
  });
</script>

<style>
  /* Dropdown container */
  .dropdown {
    display: flex;
    flex-direction: column;
    gap: 10px;
    margin-top: 10px;
    padding: 10px;
    background-color: #f9f9f9;
    border: 1px solid #ddd;
    border-radius: 8px;
    position: relative;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    animation: fadeIn 0.3s ease-in-out;
  }

  /* Hide dropdown by default */
  .hidden {
    display: none;
  }

  /* Dropdown links */
  .dropdown a {
    text-decoration: none;
    color: #007BFF;
    padding: 5px 10px;
    border-radius: 5px;
    transition: background-color 0.2s ease;
  }

  .dropdown a:hover {
    background-color: #e6f0ff;
  }

  /* Fade-in animation */
  @keyframes fadeIn {
    from {
      opacity: 0;
      transform: translateY(-10px);
    }
    to {
      opacity: 1;
      transform: translateY(0);
    }
  }
  .paper-container {
    display: flex;
    align-items: center;
  }

  .paper-video {
    width: 400px;
    height: 250px;
    margin-right: 20px;
  }

  .paper-video video {
    width: 100%;
    height: 100%;
  }

  .paper-description {
    max-width: 600px;
  }

  /* Hide the controls */
  video::-webkit-media-controls {
    display: none !important;
  }

  video::-webkit-media-controls-play-button {
    display: none !important;
  }

  video::-webkit-media-controls-start-playback-button {
    display: none !important;
  }
</style>

<!-- Publications Section -->
<div class="section">
  <h2>Publications</h2>
  <div class="paper-container">
    <div class="paper-video">
      <!-- Video that starts playing on page load -->
      <video id="splatSimVideo" muted loop>
        <source src="figures/icra.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>

    <div class="paper-description">
      <p><b id="papertitle">SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting</b> <br/>
      Mohammad Nomaan Qureshi, <strong>Sparsh Garg</strong>, Francisco Yandun, David Held, George Kantor, Abhisesh Silwal<br />
      (Accepted to ICRA 2025) (Spotlight Presentation at CoRL MRM-D Workshop)<br />
      <a href="https://www.arxiv.org/abs/2409.10161">pdf</a>  &nbsp 
      <a href="https://splatsim.github.io/">project page</a>  &nbsp 
      <a href="javascript:toggleblock('splatSim2024Abs')">abstract</a> &nbsp 
      <a href="javascript:toggleblock('splatSim2024Bib')">bibtex</a> 
      </p>

      <div class="papermeta" id="splatSim2024Meta">
        <em id="splatSim2024Abs">Sim2Real transfer, particularly for manipulation
        policies relying on RGB images, remains a critical challenge
        in robotics due to the significant domain shift between syn-
        thetic and real-world visual data. In this paper, we propose
        SplatSim, a novel framework that leverages Gaussian Splatting
        as the primary rendering primitive to reduce the Sim2Real
        gap for RGB-based manipulation policies. By replacing tradi-
        tional mesh representations with Gaussian Splats in simulators,
        SplatSim produces highly photorealistic synthetic data while
        maintaining the scalability and cost-efficiency of simulation.
        We demonstrate the effectiveness of our framework by training
        manipulation policies within SplatSim and deploying them in
        the real world in a zero-shot manner, achieving an average
        success rate of 86.25%, compared to 97.5% for policies trained
        on real-world data.</em>
        <pre xml:space="preserve" id="splatSim2024Bib" style="font-size: 12px">
@misc{qureshi2024splatsimzeroshotsim2realtransfer,
      title={SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using Gaussian Splatting}, 
      author={Mohammad Nomaan Qureshi and Sparsh Garg and Francisco Yandun and David Held and George Kantor and Abhisesh Silwal},
      year={2024},
      eprint={2409.10161},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2409.10161}, 
}
        </pre>
      </div>

      <script language="javascript" type="text/javascript" xml:space="preserve">
        hideblock('splatSim2024Abs');
        hideblock('splatSim2024Bib');
      </script>
    </div>
  </div>

  <!-- Depth Any Camera Publication -->
  <div class="paper-container" style="display: flex; gap: 20px; align-items: flex-start; margin-bottom: 20px;">
    <div class="paper-video">
      <!-- Placeholder Image -->
      <img src="figures/dac_teaser.png" alt="Depth Any Camera Visualization" style="width: 100%; max-width: 400px; border-radius: 8px;">
    </div>
  
    <div class="paper-description" style="margin: 0; padding: 0;">
      <p style="margin: 0;">
        <b id="papertitle">Depth Any Camera (DAC): A Framework for Zero-Shot Metric Depth Estimation Across Diverse Camera Types</b> <br />
        Yuliang Guo*, <strong>Sparsh Garg*</strong>, S. Mahdi H. Miangoleh, Xinyu Huang, Liu Ren <br />
        (Under Review at CVPR 2025) <br />
        <a href="https://drive.google.com/file/d/1yhUKVjfRbV_kd395rlVXSmNC2-8rIzMf/view?usp=sharing">pdf</a> &nbsp; 
        <a href="#">project page (Coming Soon)</a> &nbsp;
        <a href="javascript:toggleblock('dacAbstract')">abstract</a>
      </p>
  
      <div class="papermeta" id="dacMeta">
        <em id="dacAbstract" style="display: none;">
          Accurate metric depth estimation from monocular cameras is essential for applications such as autonomous driving, AR/VR, and robotics. While recent depth estimation methods demonstrate strong zero-shot generalization, achieving accurate metric depth across diverse camera typesâ€”particularly those with large fields of view (FoV) like fisheye and 360-degree camerasâ€”remains challenging. This paper introduces <b>Depth Any Camera (DAC)</b>, a novel zero-shot metric depth estimation framework that extends a perspective-trained model to handle varying FoVs effectively. Notably, DAC is trained exclusively on perspective images, yet it generalizes seamlessly to fisheye and 360-degree cameras without requiring specialized training. DAC leverages Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. Key components include an efficient Image-to-ERP patch conversion for seamless online augmentation in ERP space, a FoV alignment operation to support effective training across a broad range of FoVs, and multi-resolution data augmentation to address resolution discrepancies between training and testing. DAC achieves state-of-the-art zero-shot metric depth estimation, improving delta_1 accuracy by up to 50% on multiple indoor fisheye and 360-degree datasets, demonstrating robust generalization across camera types while relying only on perspective training data.
        </em>
      </div>
  
      <script>
        function toggleblock(id) {
          const block = document.getElementById(id);
          block.style.display = block.style.display === "block" ? "none" : "block";
        }
      </script>
    </div>
</div>

<!-- JavaScript to autoplay the video when the page loads -->
<script type="text/javascript">
  window.onload = function() {
    const video = document.getElementById('splatSimVideo');
    video.play();
  };
</script>




</body>
</html>
